"""å°†æå–çš„è¡¨æ ¼å¯¼å‡ºä¸ºCSVæ–‡ä»¶

åŠŸèƒ½ï¼š
1. è¯»å–JSONæ ¼å¼çš„æå–ç»“æœ
2. å°†æ¯ä¸ªè¡¨æ ¼å¯¼å‡ºä¸ºç‹¬ç«‹çš„CSVæ–‡ä»¶
3. ç”Ÿæˆè¡¨æ ¼ç´¢å¼•æ–‡ä»¶ï¼ˆåŒ…å«è¡¨æ ¼å…ƒæ•°æ®ï¼‰
4. å¯é€‰ï¼šå°†åŒä¸€PDFçš„æ‰€æœ‰è¡¨æ ¼å¯¼å‡ºåˆ°Excelçš„å¤šä¸ªsheet
"""
import json
import sys
from pathlib import Path
from datetime import datetime
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# å¯¼å…¥é…ç½®å’Œæ—¥å¿—
from src.utils.config import settings
from src.utils.logger import setup_script_logger
from src.ingestion.table_classifier import TableClassifier

# è®¾ç½®æ—¥å¿—
logger = setup_script_logger("export")


def extract_company_name(document_id: str) -> str:
    """ä»æ–‡æ¡£IDä¸­æå–å…¬å¸ç®€ç§°

    Args:
        document_id: æ–‡æ¡£ID

    Returns:
        å…¬å¸ç®€ç§°
    """
    # æå–è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°ï¼ˆé€šå¸¸åœ¨æ–‡æ¡£IDå¼€å¤´ï¼‰
    # ä¾‹å¦‚: "000035ä¸­å›½å¤©æ¥¹..." -> "000035-ä¸­å›½å¤©æ¥¹"
    import re
    match = re.match(r'(\d+)([^å‘è¡Œè´­ä¹°]+)', document_id)
    if match:
        code, name = match.groups()
        # æˆªå–å…¬å¸åç§°å‰å‡ ä¸ªå­—
        name = name[:10] if len(name) > 10 else name
        return f"{code}-{name}"
    return document_id[:20]


def find_table_title(text_chunks: list, table_page: int) -> str:
    """ä»æ–‡æœ¬å—ä¸­æŸ¥æ‰¾è¡¨æ ¼æ ‡é¢˜

    Args:
        text_chunks: æ–‡æœ¬å—åˆ—è¡¨
        table_page: è¡¨æ ¼æ‰€åœ¨é¡µç 

    Returns:
        è¡¨æ ¼æ ‡é¢˜ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰ï¼Œå¦åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    # åœ¨è¡¨æ ¼æ‰€åœ¨é¡µæŸ¥æ‰¾å¯èƒ½çš„æ ‡é¢˜
    for chunk in text_chunks:
        if chunk.get('page') == table_page:
            text = chunk.get('text', '')
            lines = text.split('\n')

            # æŸ¥æ‰¾å¯èƒ½çš„æ ‡é¢˜ç‰¹å¾ï¼š
            # 1. çŸ­è¡Œï¼ˆå°‘äº30å­—ç¬¦ï¼‰
            # 2. åŒ…å«"è¡¨"ã€"ä¸€è§ˆ"ã€"æ˜ç»†"ã€"æƒ…å†µ"ç­‰å…³é”®è¯
            # 3. æˆ–è€…æ˜¯"é‡Šä¹‰"ã€"å£°æ˜"ç­‰ç‰¹æ®Šæ ‡é¢˜
            title_keywords = ['è¡¨', 'ä¸€è§ˆ', 'æ˜ç»†', 'æƒ…å†µ', 'åˆ—è¡¨', 'æ¸…å•', 'æ±‡æ€»', 'ç»Ÿè®¡',
                            'é‡Šä¹‰', 'å£°æ˜', 'è¯´æ˜', 'æ¦‚å†µ', 'ä¿¡æ¯', 'æ•°æ®', 'èµ„æ–™']

            # æ’é™¤çš„é¡µçœ‰ç‰¹å¾
            exclude_keywords = ['å…¬å¸', 'è‚¡ä»½æœ‰é™å…¬å¸', 'æŠ¥å‘Šä¹¦', 'å…¬å‘Šä¹¦', 'æ‘˜è¦']

            candidates = []
            for line in lines:
                line = line.strip()
                # è·³è¿‡è¿‡é•¿æˆ–è¿‡çŸ­çš„è¡Œ
                if 2 <= len(line) <= 30:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ ‡é¢˜å…³é”®è¯
                    has_title_keyword = any(kw in line for kw in title_keywords)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯é¡µçœ‰
                    is_header = all(kw in line for kw in exclude_keywords[:2])

                    if has_title_keyword and not is_header:
                        # æ¸…ç†æ ‡é¢˜
                        line = line.replace('\n', '').replace('\r', '').strip()
                        candidates.append(line)

            # è¿”å›ç¬¬ä¸€ä¸ªå€™é€‰æ ‡é¢˜
            if candidates:
                return candidates[0]

            # å¦‚æœæ²¡æ‰¾åˆ°åˆé€‚çš„ï¼Œè¿”å›ç©ºï¼ˆåç»­ç”¨é»˜è®¤å‘½åï¼‰
            return ""

    return ""


def generate_smart_filename(
    table_info: dict,
    classification: dict,
    context_title: str,
    page: int,
    company_code: str,
    existing_names: set
) -> str:
    """ç”Ÿæˆæ™ºèƒ½æ–‡ä»¶å

    Args:
        table_info: è¡¨æ ¼ä¿¡æ¯
        classification: åˆ†ç±»ç»“æœ
        context_title: ä»ä¸Šä¸‹æ–‡æå–çš„æ ‡é¢˜
        page: é¡µç 
        company_code: å…¬å¸ä»£ç 
        existing_names: å·²å­˜åœ¨çš„æ–‡ä»¶åé›†åˆï¼ˆç”¨äºå¤„ç†é‡åï¼‰

    Returns:
        æ¸…ç†åçš„æ–‡ä»¶å
    """
    # ç­–ç•¥1: ä¼˜å…ˆä½¿ç”¨ä¸Šä¸‹æ–‡æ ‡é¢˜
    if context_title and len(context_title) > 3:
        base_name = context_title
    # ç­–ç•¥2: ä½¿ç”¨åˆ†ç±»ç»“æœç”Ÿæˆæ ‡é¢˜
    elif classification.get('type') != 'other':
        category_map = {
            'balance_sheet': 'èµ„äº§è´Ÿå€ºè¡¨',
            'income_statement': 'åˆ©æ¶¦è¡¨',
            'cash_flow_statement': 'ç°é‡‘æµé‡è¡¨',
            'equity_statement': 'æ‰€æœ‰è€…æƒç›Šå˜åŠ¨è¡¨',
            'usage': 'å‹Ÿé›†èµ„é‡‘ä½¿ç”¨æƒ…å†µ',
            'source': 'å‹Ÿé›†èµ„é‡‘æ¥æº',
            'issuance': 'å‘è¡Œæ–¹æ¡ˆ'
        }
        base_name = category_map.get(
            classification.get('category'),
            classification.get('suggested_title', '')
        )
    # ç­–ç•¥3: ä½¿ç”¨table_id
    else:
        base_name = table_info.get('table_id', 'table')

    # æ¸…ç†æ–‡ä»¶å
    base_name = sanitize_filename(base_name)

    # é™åˆ¶é•¿åº¦
    if len(base_name) > 50:
        base_name = base_name[:50]

    # ç”Ÿæˆå®Œæ•´æ–‡ä»¶å
    filename = f"{base_name}_page{page}_{company_code}.csv"

    # å¤„ç†é‡å
    if filename in existing_names:
        counter = 2
        while f"{base_name}_{counter}_page{page}_{company_code}.csv" in existing_names:
            counter += 1
        filename = f"{base_name}_{counter}_page{page}_{company_code}.csv"

    return filename


def sanitize_filename(filename: str) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦

    Args:
        filename: åŸå§‹æ–‡ä»¶å

    Returns:
        æ¸…ç†åçš„æ–‡ä»¶å
    """
    # ç§»é™¤æ–‡ä»¶ç³»ç»Ÿä¸å…è®¸çš„å­—ç¬¦
    illegal_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|', '\n', '\r']
    for char in illegal_chars:
        filename = filename.replace(char, '')

    # é™åˆ¶é•¿åº¦
    if len(filename) > 80:
        filename = filename[:80]

    return filename.strip()


def export_tables_from_json(json_file: Path, output_dir: Path, export_format: str = "csv"):
    """ä»JSONæ–‡ä»¶å¯¼å‡ºè¡¨æ ¼

    Args:
        json_file: JSONæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        export_format: å¯¼å‡ºæ ¼å¼ ("csv" æˆ– "excel")

    Returns:
        å¯¼å‡ºçš„è¡¨æ ¼æ•°é‡
    """
    # è¯»å–JSON
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    document_id = data.get("document_id", "unknown")
    tables = data.get("tables", [])
    text_chunks = data.get("text_chunks", [])

    if not tables:
        logger.warning(f"{json_file.name}: æ²¡æœ‰è¡¨æ ¼æ•°æ®")
        return 0

    # æå–å…¬å¸ä»£ç 
    company_code = extract_company_name(document_id)

    # åˆ›å»ºæ–‡æ¡£ä¸“å±ç›®å½•
    doc_dir = output_dir / document_id
    doc_dir.mkdir(parents=True, exist_ok=True)

    # åˆå§‹åŒ–åˆ†ç±»å™¨
    classifier = TableClassifier()

    # è¡¨æ ¼ç´¢å¼•ä¿¡æ¯
    table_index = []
    existing_names = set()  # ç”¨äºå¤„ç†é‡å

    # å¯¼å‡ºæ¯ä¸ªè¡¨æ ¼
    exported_count = 0
    for i, table in enumerate(tables, 1):
        try:
            table_id = table.get("table_id", f"table_{i}")
            page = table.get("page", "unknown")
            accuracy = table.get("accuracy", "N/A")
            table_data = table.get("data", {})

            if not table_data:
                logger.warning(f"è¡¨æ ¼ {table_id} (é¡µ{page}): æ•°æ®ä¸ºç©º")
                continue

            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(table_data)

            # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
            if df.empty:
                logger.warning(f"è¡¨æ ¼ {table_id} (é¡µ{page}): DataFrameä¸ºç©º")
                continue

            # æ¸…ç†å•å…ƒæ ¼å†…çš„æ¢è¡Œç¬¦ï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ¸…ç†ï¼‰
            if settings.export_clean_newlines:
                df = df.map(lambda x: str(x).replace('\n', '').replace('\r', '').strip() if pd.notna(x) else x)

            # è¡¨æ ¼åˆ†ç±»å’Œæ ‡é¢˜æå–
            context_title = classifier.extract_title_from_context(text_chunks, page, i)
            classification = classifier.classify_table(df, context_text=context_title or "")

            # ç”Ÿæˆæ™ºèƒ½æ–‡ä»¶å
            filename = generate_smart_filename(
                table_info={'table_id': table_id},
                classification=classification,
                context_title=context_title,
                page=page,
                company_code=company_code,
                existing_names=existing_names
            )
            existing_names.add(filename)

            # å¯¼å‡ºCSV
            if export_format == "csv":
                csv_file = doc_dir / filename
                df.to_csv(csv_file, index=False, encoding='utf-8-sig')
                exported_count += 1

                # è®°å½•ç´¢å¼•ä¿¡æ¯
                table_index.append({
                    "è¡¨æ ¼ID": table_id,
                    "æ ‡é¢˜": context_title or classification.get('suggested_title', ''),
                    "ç±»å‹": classification.get('type', 'unknown'),
                    "ç±»åˆ«": classification.get('category', 'unknown'),
                    "ç½®ä¿¡åº¦": f"{classification.get('confidence', 0):.2f}",
                    "é¡µç ": page,
                    "å‡†ç¡®åº¦": accuracy,
                    "è¡Œæ•°": len(df),
                    "åˆ—æ•°": len(df.columns),
                    "æ–‡ä»¶å": filename
                })

                logger.debug(f"å¯¼å‡º: {filename} (ç±»å‹: {classification.get('type')})")

        except Exception as e:
            logger.error(f"è¡¨æ ¼ {table_id} å¯¼å‡ºå¤±è´¥: {e}")
            continue

    # å¯¼å‡ºè¡¨æ ¼ç´¢å¼•
    if table_index:
        index_df = pd.DataFrame(table_index)
        index_file = doc_dir / "_table_index.csv"
        index_df.to_csv(index_file, index=False, encoding='utf-8-sig')
        logger.info(f"{document_id}: å¯¼å‡º {exported_count}/{len(tables)} ä¸ªè¡¨æ ¼")

    return exported_count


def main():
    """ä¸»å‡½æ•°"""
    # ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
    results_dir = Path(settings.output_dir) / "test_results"
    export_dir = Path(settings.output_dir) / "tables_csv"
    export_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"å¼€å§‹å¯¼å‡ºè¡¨æ ¼ï¼Œè¾“å…¥ç›®å½•: {results_dir}, è¾“å‡ºç›®å½•: {export_dir}")

    print(f"\n{'='*60}")
    print(f"è¡¨æ ¼CSVå¯¼å‡ºå·¥å…·")
    print(f"{'='*60}")
    print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {results_dir}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {export_dir}")
    print(f"{'='*60}\n")

    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    json_files = list(results_dir.glob("*_extracted.json"))

    if not json_files:
        logger.warning(f"åœ¨ {results_dir} ä¸­æœªæ‰¾åˆ°æå–ç»“æœæ–‡ä»¶")
        print(f"âŒ åœ¨ {results_dir} ä¸­æœªæ‰¾åˆ°æå–ç»“æœæ–‡ä»¶")
        return

    logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")

    print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶\n")

    # ç»Ÿè®¡ä¿¡æ¯
    total_tables = 0
    total_exported = 0
    start_time = datetime.now()

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for json_file in json_files:
        try:
            # è¯»å–è¡¨æ ¼æ•°é‡
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                table_count = len(data.get("tables", []))
                total_tables += table_count

            # å¯¼å‡ºè¡¨æ ¼
            exported = export_tables_from_json(json_file, export_dir)
            total_exported += exported

        except Exception as e:
            logger.error(f"{json_file.name}: å¤„ç†å¤±è´¥ - {e}")

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    logger.info(f"å¯¼å‡ºå®Œæˆï¼Œæ€»è¡¨æ ¼æ•°: {total_tables}, æˆåŠŸå¯¼å‡º: {total_exported}, è€—æ—¶: {duration:.1f}ç§’")

    print(f"\n{'='*60}")
    print(f"å¯¼å‡ºå®Œæˆ")
    print(f"{'='*60}")
    print(f"ğŸ“Š æ€»è¡¨æ ¼æ•°: {total_tables}")
    print(f"âœ… æˆåŠŸå¯¼å‡º: {total_exported}")
    print(f"âŒ å¤±è´¥/è·³è¿‡: {total_tables - total_exported}")
    print(f"â±ï¸  è€—æ—¶: {duration:.1f}ç§’")
    print(f"ğŸ“‚ è¾“å‡ºä½ç½®: {export_dir.absolute()}")
    print(f"{'='*60}\n")

    # ç”Ÿæˆå…¨å±€ç´¢å¼•
    print("ğŸ“ ç”Ÿæˆå…¨å±€è¡¨æ ¼ç´¢å¼•...")
    all_indexes = []
    for doc_dir in export_dir.iterdir():
        if doc_dir.is_dir():
            index_file = doc_dir / "_table_index.csv"
            if index_file.exists():
                df = pd.read_csv(index_file, encoding='utf-8-sig')
                df['æ–‡æ¡£ID'] = doc_dir.name
                all_indexes.append(df)

    if all_indexes:
        global_index = pd.concat(all_indexes, ignore_index=True)
        global_index_file = export_dir / "å…¨å±€è¡¨æ ¼ç´¢å¼•.csv"
        global_index.to_csv(global_index_file, index=False, encoding='utf-8-sig')
        print(f"âœ… å…¨å±€ç´¢å¼•å·²ä¿å­˜: {global_index_file}")

        # æ˜¾ç¤ºç»Ÿè®¡
        print(f"\nğŸ“ˆ è¡¨æ ¼ç»Ÿè®¡:")
        print(f"  æ€»æ–‡æ¡£æ•°: {global_index['æ–‡æ¡£ID'].nunique()}")
        print(f"  æ€»è¡¨æ ¼æ•°: {len(global_index)}")
        print(f"  å¹³å‡å‡†ç¡®åº¦: {global_index['å‡†ç¡®åº¦'].mean():.2f}%")
        print(f"  å¹³å‡è¡Œæ•°: {global_index['è¡Œæ•°'].mean():.1f}")
        print(f"  å¹³å‡åˆ—æ•°: {global_index['åˆ—æ•°'].mean():.1f}")


if __name__ == "__main__":
    main()
