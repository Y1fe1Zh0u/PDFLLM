"""åˆå¹¶è·¨é¡µè¡¨æ ¼

åŠŸèƒ½ï¼š
1. æ£€æµ‹è¿ç»­é¡µé¢ä¸Šçš„ç›¸ä¼¼è¡¨æ ¼
2. åŸºäºåˆ—æ•°å’Œè¡¨å¤´ç›¸ä¼¼åº¦åˆ¤æ–­æ˜¯å¦ä¸ºè·¨é¡µè¡¨æ ¼
3. åˆå¹¶è·¨é¡µè¡¨æ ¼å¹¶æ›´æ–°ç´¢å¼•
"""
import json
import sys
from pathlib import Path
from datetime import datetime
import pandas as pd
from difflib import SequenceMatcher

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# å¯¼å…¥é…ç½®å’Œæ—¥å¿—
from src.utils.config import settings
from src.utils.logger import setup_script_logger

# è®¾ç½®æ—¥å¿—
logger = setup_script_logger("merge")


def calculate_similarity(text1: str, text2: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰"""
    if not text1 or not text2:
        return 0.0
    return SequenceMatcher(None, str(text1), str(text2)).ratio()


def check_header_similarity(df1: pd.DataFrame, df2: pd.DataFrame, threshold: float = 0.7) -> bool:
    """æ£€æŸ¥ä¸¤ä¸ªè¡¨æ ¼çš„è¡¨å¤´ç›¸ä¼¼åº¦

    Args:
        df1: ç¬¬ä¸€ä¸ªè¡¨æ ¼
        df2: ç¬¬äºŒä¸ªè¡¨æ ¼
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰

    Returns:
        æ˜¯å¦ç›¸ä¼¼
    """
    if df1.empty or df2.empty:
        return False

    # æ¯”è¾ƒç¬¬ä¸€è¡Œï¼ˆé€šå¸¸æ˜¯è¡¨å¤´ï¼‰
    header1 = ' '.join([str(x) for x in df1.iloc[0].tolist()])
    header2 = ' '.join([str(x) for x in df2.iloc[0].tolist()])

    similarity = calculate_similarity(header1, header2)
    return similarity >= threshold


def looks_like_header(row: pd.Series) -> bool:
    """åˆ¤æ–­ä¸€è¡Œæ˜¯å¦çœ‹èµ·æ¥åƒè¡¨å¤´

    Args:
        row: DataFrameçš„ä¸€è¡Œ

    Returns:
        æ˜¯å¦åƒè¡¨å¤´
    """
    # è¡¨å¤´ç‰¹å¾å…³é”®è¯
    header_keywords = [
        'åç§°', 'ç¼–å·', 'åºå·', 'é¡¹ç›®', 'å†…å®¹', 'é‡‘é¢', 'æ•°é‡', 'å•ä½', 'æ—¥æœŸ',
        'ç±»å‹', 'è¯´æ˜', 'å¤‡æ³¨', 'åˆè®¡', 'å°è®¡', 'æ¯”ä¾‹', 'å æ¯”', 'å¹´åº¦', 'æœŸé—´',
        'Name', 'No', 'Item', 'Amount', 'Date', 'Type', 'Total', 'Ratio'
    ]

    row_str = ' '.join([str(x) for x in row.tolist()])

    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨å¤´å…³é”®è¯
    keyword_count = sum(1 for keyword in header_keywords if keyword in row_str)

    # å¦‚æœåŒ…å«2ä¸ªä»¥ä¸Šå…³é”®è¯ï¼Œå¯èƒ½æ˜¯è¡¨å¤´
    if keyword_count >= 2:
        return True

    # æ£€æŸ¥æ˜¯å¦å¤§éƒ¨åˆ†æ˜¯æ•°å­—ï¼ˆå¦‚æœæ˜¯ï¼Œåˆ™ä¸åƒè¡¨å¤´ï¼‰
    numeric_count = 0
    for val in row.tolist():
        val_str = str(val).strip()
        # å°è¯•è½¬æ¢ä¸ºæ•°å­—
        try:
            float(val_str.replace(',', '').replace('%', ''))
            numeric_count += 1
        except:
            pass

    # å¦‚æœè¶…è¿‡ä¸€åŠæ˜¯æ•°å­—ï¼Œä¸åƒè¡¨å¤´
    if numeric_count > len(row) / 2:
        return False

    return False


def should_merge_tables(table1: dict, table2: dict, similarity_threshold: float = 0.7) -> tuple:
    """åˆ¤æ–­ä¸¤ä¸ªè¡¨æ ¼æ˜¯å¦åº”è¯¥åˆå¹¶

    Args:
        table1: ç¬¬ä¸€ä¸ªè¡¨æ ¼ä¿¡æ¯ï¼ˆåŒ…å«page, dataç­‰ï¼‰
        table2: ç¬¬äºŒä¸ªè¡¨æ ¼ä¿¡æ¯
        similarity_threshold: è¡¨å¤´ç›¸ä¼¼åº¦é˜ˆå€¼

    Returns:
        (æ˜¯å¦åº”è¯¥åˆå¹¶, åˆå¹¶ç±»å‹)
        åˆå¹¶ç±»å‹: "header_repeat" | "data_continuation" | None
    """
    # 1. æ£€æŸ¥é¡µç æ˜¯å¦è¿ç»­
    if table2["page"] - table1["page"] != 1:
        return False, None

    # 2. æ£€æŸ¥åˆ—æ•°æ˜¯å¦ç›¸åŒ
    df1 = pd.DataFrame(table1["data"])
    df2 = pd.DataFrame(table2["data"])

    if len(df1.columns) != len(df2.columns):
        return False, None

    # 3. æ£€æŸ¥è¡¨å¤´ç›¸ä¼¼åº¦
    has_similar_headers = check_header_similarity(df1, df2, similarity_threshold)

    if has_similar_headers:
        # æ¨¡å¼1: é‡å¤è¡¨å¤´æ¨¡å¼
        return True, "header_repeat"

    # 4. æ£€æŸ¥æ˜¯å¦ä¸ºæ•°æ®å»¶ç»­æ¨¡å¼
    # å¦‚æœç¬¬äºŒä¸ªè¡¨æ ¼çš„ç¬¬ä¸€è¡Œä¸åƒè¡¨å¤´ï¼Œä¸”è¡¨å¤´ä¸ç›¸ä¼¼ï¼Œåˆ™å¯èƒ½æ˜¯æ•°æ®å»¶ç»­
    if not df2.empty:
        first_row_df2 = df2.iloc[0]
        if not looks_like_header(first_row_df2):
            # æ¨¡å¼2: æ•°æ®å»¶ç»­æ¨¡å¼
            return True, "data_continuation"

    return False, None


def merge_tables(tables: list, merge_type: str = "header_repeat") -> pd.DataFrame:
    """åˆå¹¶å¤šä¸ªè¡¨æ ¼

    Args:
        tables: è¡¨æ ¼åˆ—è¡¨ï¼Œæ¯ä¸ªè¡¨æ ¼åŒ…å« data å­—æ®µ
        merge_type: åˆå¹¶ç±»å‹ ("header_repeat" æˆ– "data_continuation")

    Returns:
        åˆå¹¶åçš„DataFrame
    """
    dfs = []
    for i, table in enumerate(tables):
        df = pd.DataFrame(table["data"])

        # ç¬¬ä¸€ä¸ªè¡¨æ ¼ä¿ç•™å®Œæ•´å†…å®¹
        if i == 0:
            dfs.append(df)
            continue

        if not df.empty:
            # é‡å¤è¡¨å¤´æ¨¡å¼ï¼šå»æ‰åç»­è¡¨æ ¼çš„è¡¨å¤´ï¼ˆç¬¬ä¸€è¡Œï¼‰
            if merge_type == "header_repeat":
                df = df.iloc[1:]
            # æ•°æ®å»¶ç»­æ¨¡å¼ï¼šä¿ç•™æ‰€æœ‰è¡Œ
            elif merge_type == "data_continuation":
                pass  # ä¿ç•™æ‰€æœ‰è¡Œ

            dfs.append(df)

    # åˆå¹¶æ‰€æœ‰è¡¨æ ¼
    merged_df = pd.concat(dfs, ignore_index=True)
    return merged_df


def process_document_tables(json_file: Path, similarity_threshold: float = 0.7,
                            dry_run: bool = False) -> dict:
    """å¤„ç†å•ä¸ªæ–‡æ¡£çš„è¡¨æ ¼åˆå¹¶

    Args:
        json_file: JSONæ–‡ä»¶è·¯å¾„
        similarity_threshold: è¡¨å¤´ç›¸ä¼¼åº¦é˜ˆå€¼
        dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œï¼ˆä¸å®é™…åˆå¹¶ï¼Œåªè¾“å‡ºæŠ¥å‘Šï¼‰

    Returns:
        åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
    """
    # è¯»å–JSON
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    document_id = data.get("document_id", "unknown")
    tables = data.get("tables", [])

    if not tables:
        return {"document_id": document_id, "merged_count": 0, "groups": []}

    print(f"\n{'='*60}")
    print(f"å¤„ç†æ–‡æ¡£: {document_id}")
    print(f"åŸå§‹è¡¨æ ¼æ•°: {len(tables)}")
    print(f"{'='*60}")

    # æ£€æµ‹éœ€è¦åˆå¹¶çš„è¡¨æ ¼ç»„
    merge_groups = []
    current_group = [tables[0]]
    current_merge_type = None

    for i in range(1, len(tables)):
        prev_table = tables[i-1]
        curr_table = tables[i]

        should_merge, merge_type = should_merge_tables(prev_table, curr_table, similarity_threshold)

        if should_merge:
            current_group.append(curr_table)
            if current_merge_type is None:
                current_merge_type = merge_type

            type_label = "é‡å¤è¡¨å¤´" if merge_type == "header_repeat" else "æ•°æ®å»¶ç»­"
            print(f"âœ“ æ£€æµ‹åˆ°è·¨é¡µè¡¨æ ¼({type_label}): {prev_table['table_id']}(é¡µ{prev_table['page']}) + {curr_table['table_id']}(é¡µ{curr_table['page']})")
        else:
            # å½“å‰ç»„ç»“æŸ
            if len(current_group) > 1:
                merge_groups.append((current_group, current_merge_type))
            current_group = [curr_table]
            current_merge_type = None

    # æ£€æŸ¥æœ€åä¸€ç»„
    if len(current_group) > 1:
        merge_groups.append((current_group, current_merge_type))

    print(f"\nå‘ç° {len(merge_groups)} ä¸ªéœ€è¦åˆå¹¶çš„è¡¨æ ¼ç»„")

    if dry_run:
        print("\n[è¯•è¿è¡Œæ¨¡å¼] ä¸æ‰§è¡Œå®é™…åˆå¹¶")
        return {
            "document_id": document_id,
            "original_count": len(tables),
            "merged_groups": len(merge_groups),
            "groups": merge_groups
        }

    # æ‰§è¡Œåˆå¹¶
    merged_tables = []
    merged_indices = set()

    for group, merge_type in merge_groups:
        # åˆå¹¶è¡¨æ ¼
        merged_df = merge_tables(group, merge_type)

        # è®°å½•åˆå¹¶ä¿¡æ¯
        start_page = group[0]["page"]
        end_page = group[-1]["page"]
        table_ids = [t["table_id"] for t in group]

        merged_table = {
            "table_id": f"{group[0]['table_id']}_merged",
            "page": f"{start_page}-{end_page}",
            "type": "table",
            "accuracy": sum(t["accuracy"] for t in group) / len(group),
            "data": merged_df.to_dict(),
            "merged_from": table_ids,
            "merged_pages": [t["page"] for t in group],
            "merge_type": merge_type
        }

        merged_tables.append(merged_table)

        # è®°å½•å·²åˆå¹¶çš„è¡¨æ ¼ç´¢å¼•
        for t in group:
            merged_indices.add(tables.index(t))

        type_label = "é‡å¤è¡¨å¤´" if merge_type == "header_repeat" else "æ•°æ®å»¶ç»­"
        print(f"âœ… åˆå¹¶({type_label}): {' + '.join(table_ids)} â†’ {merged_table['table_id']} (é¡µ{start_page}-{end_page})")

    # ä¿ç•™æœªåˆå¹¶çš„è¡¨æ ¼
    final_tables = []
    for i, table in enumerate(tables):
        if i not in merged_indices:
            final_tables.append(table)

    # æ·»åŠ åˆå¹¶åçš„è¡¨æ ¼
    final_tables.extend(merged_tables)

    # æŒ‰é¡µç æ’åº
    final_tables.sort(key=lambda x: int(str(x["page"]).split('-')[0]))

    # æ›´æ–°JSON
    data["tables"] = final_tables
    data["merge_info"] = {
        "original_count": len(tables),
        "merged_count": len(merged_tables),
        "final_count": len(final_tables),
        "merge_timestamp": datetime.now().isoformat()
    }

    # ä¿å­˜æ›´æ–°åçš„JSON
    output_file = json_file.parent / f"{json_file.stem}_merged.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ åˆå¹¶åè¡¨æ ¼æ•°: {len(final_tables)}")
    print(f"ğŸ’¾ å·²ä¿å­˜: {output_file.name}")

    return {
        "document_id": document_id,
        "original_count": len(tables),
        "merged_groups": len(merge_groups),
        "final_count": len(final_tables)
    }


def main():
    """ä¸»å‡½æ•°"""
    # ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„å’Œå‚æ•°
    results_dir = Path(settings.output_dir) / "test_results"
    similarity_threshold = settings.table_merge_similarity_threshold
    dry_run = settings.table_merge_dry_run

    logger.info(f"å¼€å§‹è·¨é¡µè¡¨æ ¼åˆå¹¶ï¼Œè¾“å…¥ç›®å½•: {results_dir}, ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}, è¯•è¿è¡Œ: {dry_run}")

    print(f"\n{'='*60}")
    print(f"è·¨é¡µè¡¨æ ¼åˆå¹¶å·¥å…·")
    print(f"{'='*60}")
    print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {results_dir}")
    print(f"ğŸ¯ è¡¨å¤´ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
    print(f"ğŸ” æ¨¡å¼: {'è¯•è¿è¡Œ' if dry_run else 'å®é™…åˆå¹¶'}")
    print(f"{'='*60}")

    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    json_files = list(results_dir.glob("*_extracted.json"))

    if not json_files:
        logger.warning(f"åœ¨ {results_dir} ä¸­æœªæ‰¾åˆ°æå–ç»“æœæ–‡ä»¶")
        print(f"âŒ åœ¨ {results_dir} ä¸­æœªæ‰¾åˆ°æå–ç»“æœæ–‡ä»¶")
        return

    logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")

    print(f"\nğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")

    # ç»Ÿè®¡ä¿¡æ¯
    total_stats = {
        "processed_docs": 0,
        "total_original": 0,
        "total_merged_groups": 0,
        "total_final": 0
    }

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for json_file in json_files:
        try:
            stats = process_document_tables(json_file, similarity_threshold, dry_run)

            total_stats["processed_docs"] += 1
            total_stats["total_original"] += stats.get("original_count", 0)
            total_stats["total_merged_groups"] += stats.get("merged_groups", 0)
            total_stats["total_final"] += stats.get("final_count", 0)

        except Exception as e:
            logger.error(f"{json_file.name}: å¤„ç†å¤±è´¥ - {e}")

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    logger.info(f"åˆå¹¶æ±‡æ€» - å¤„ç†æ–‡æ¡£æ•°: {total_stats['processed_docs']}, æ£€æµ‹åˆ°è·¨é¡µç»„: {total_stats['total_merged_groups']}")
    print(f"\n{'='*60}")
    print(f"åˆå¹¶æ±‡æ€»")
    print(f"{'='*60}")
    print(f"ğŸ“Š å¤„ç†æ–‡æ¡£æ•°: {total_stats['processed_docs']}")
    print(f"ğŸ“‹ åŸå§‹è¡¨æ ¼æ€»æ•°: {total_stats['total_original']}")
    print(f"ğŸ”— æ£€æµ‹åˆ°çš„è·¨é¡µç»„: {total_stats['total_merged_groups']}")
    if not dry_run:
        print(f"âœ… åˆå¹¶åè¡¨æ ¼æ€»æ•°: {total_stats['total_final']}")
        print(f"ğŸ“‰ å‡å°‘è¡¨æ ¼æ•°: {total_stats['total_original'] - total_stats['total_final']}")
    print(f"{'='*60}\n")

    if dry_run:
        print("ğŸ’¡ æç¤º: è¿™æ˜¯è¯•è¿è¡Œæ¨¡å¼ï¼Œæœªæ‰§è¡Œå®é™…åˆå¹¶")
        print("   æ£€æŸ¥ä¸Šè¿°æ£€æµ‹ç»“æœï¼Œå¦‚æœæ­£ç¡®ï¼Œä¿®æ”¹è„šæœ¬ä¸­ dry_run=False åé‡æ–°è¿è¡Œ")


if __name__ == "__main__":
    main()
