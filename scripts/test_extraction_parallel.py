"""å¹¶å‘æ‰¹é‡æµ‹è¯•è´¢æŠ¥æå–è„šæœ¬ - ä¼˜åŒ–ç‰ˆ"""
import sys
import json
from pathlib import Path
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# å¯¼å…¥ç»Ÿä¸€çš„ç¯å¢ƒè®¾ç½®å’Œé…ç½®
from src.utils.env_setup import setup_ghostscript_env, ensure_env_in_subprocess
from src.utils.config import settings
from src.utils.logger import setup_script_logger
from src.ingestion.pdf_extractor import PDFExtractor

# è®¾ç½®ç¯å¢ƒï¼ˆä¸»è¿›ç¨‹ï¼‰
setup_ghostscript_env()

# è®¾ç½®æ—¥å¿—
logger = setup_script_logger("extraction")


def test_single_pdf(pdf_path: Path, output_dir: Path) -> dict:
    """æµ‹è¯•å•ä¸ªPDFæ–‡ä»¶ï¼ˆç”¨äºå¹¶å‘æ‰§è¡Œï¼‰

    Returns:
        åŒ…å«æå–ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    # åœ¨å­è¿›ç¨‹ä¸­è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå…³é”®ï¼ï¼‰
    ensure_env_in_subprocess()

    result = {
        "file_name": pdf_path.name,
        "file_size_mb": pdf_path.stat().st_size / (1024 * 1024),
        "status": "pending",
        "error": None,
        "stats": {},
        "start_time": None,
        "end_time": None,
    }

    try:
        start = datetime.now()
        result["start_time"] = start.strftime("%H:%M:%S")

        logger.info(f"å¼€å§‹å¤„ç†: {pdf_path.name}")

        # æå–
        extractor = PDFExtractor(str(pdf_path))
        data = extractor.extract_all()

        # ç»Ÿè®¡ä¿¡æ¯
        text_chunks = data.get("text_chunks", [])
        tables = data.get("tables", [])

        result["stats"] = {
            "æ€»é¡µæ•°": len(text_chunks),
            "æ–‡æœ¬å—æ•°": len(text_chunks),
            "è¡¨æ ¼æ•°": len(tables),
            "å¹³å‡æ¯é¡µæ–‡æœ¬é•¿åº¦": sum(len(c["text"]) for c in text_chunks) / len(text_chunks) if text_chunks else 0,
        }

        # è¡¨æ ¼è¯¦æƒ…
        result["stats"]["è¡¨æ ¼è¯¦æƒ…"] = []
        for i, table in enumerate(tables, 1):
            table_info = {
                "ç¼–å·": i,
                "é¡µç ": table.get("page"),
                "è¡Œæ•°": len(table.get("dataframe", [])) if table.get("dataframe") is not None and hasattr(table.get("dataframe"), "__len__") else 0,
                "å‡†ç¡®åº¦": table.get("accuracy", "N/A"),
            }
            result["stats"]["è¡¨æ ¼è¯¦æƒ…"].append(table_info)

        # ä¿å­˜æå–ç»“æœ
        output_file = output_dir / f"{pdf_path.stem}_extracted.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            # å°† DataFrame è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            serializable_data = {
                "document_id": data["document_id"],
                "source_path": data["source_path"],
                "text_chunks": data["text_chunks"],
                "tables": [
                    {
                        "table_id": t.get("table_id"),
                        "page": t.get("page"),
                        "type": t.get("type"),
                        "accuracy": t.get("accuracy"),
                        "data": t["dataframe"].to_dict() if hasattr(t.get("dataframe"), "to_dict") else t.get("dataframe"),
                    }
                    for t in tables
                ]
            }
            json.dump(serializable_data, f, ensure_ascii=False, indent=2)

        result["status"] = "success"
        result["output_file"] = str(output_file)

        end = datetime.now()
        result["end_time"] = end.strftime("%H:%M:%S")
        duration = (end - start).total_seconds()

        logger.info(f"âœ… {pdf_path.name} - æˆåŠŸ! æå–äº† {len(tables)} ä¸ªè¡¨æ ¼ (è€—æ—¶ {duration:.1f}ç§’)")

    except Exception as e:
        result["status"] = "failed"
        result["error"] = str(e)
        result["traceback"] = traceback.format_exc()
        result["end_time"] = datetime.now().strftime("%H:%M:%S")
        logger.error(f"âŒ {pdf_path.name} - å¤±è´¥: {e}")

    return result


def process_pdf_wrapper(args):
    """åŒ…è£…å‡½æ•°ç”¨äºè¿›ç¨‹æ± """
    pdf_path, output_dir = args
    return test_single_pdf(pdf_path, output_dir)


def main():
    """ä¸»å‡½æ•°ï¼šå¹¶å‘æ‰¹é‡æµ‹è¯•æ‰€æœ‰PDF"""
    # ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
    upload_dir = Path(settings.upload_dir)
    output_dir = Path(settings.output_dir) / "test_results"
    output_dir.mkdir(parents=True, exist_ok=True)

    # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
    pdf_files = list(upload_dir.glob("*.pdf"))

    if not pdf_files:
        logger.warning(f"åœ¨ {upload_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶!")
        print(f"\nâš ï¸  åœ¨ {upload_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶!")
        print(f"\nè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œ:")
        print(f"1. å°†ä½ çš„è´¢æŠ¥PDFæ–‡ä»¶å¤åˆ¶åˆ°: {upload_dir.absolute()}")
        print(f"2. é‡æ–°è¿è¡Œæ­¤è„šæœ¬: python scripts/test_extraction_parallel.py")
        return

    # ä½¿ç”¨é…ç½®ä¸­çš„å¹¶å‘æ•°
    max_workers = min(cpu_count(), len(pdf_files), settings.max_workers)

    logger.info(f"æ‰¾åˆ° {len(pdf_files)} ä»½PDFæ–‡ä»¶ï¼Œä½¿ç”¨ {max_workers} ä¸ªworkerå¹¶å‘å¤„ç†")

    print(f"\n{'='*60}")
    print(f"å¹¶å‘æå–æµ‹è¯•")
    print(f"{'='*60}")
    print(f"ğŸ“ æ‰¾åˆ° {len(pdf_files)} ä»½PDFæ–‡ä»¶")
    print(f"ğŸš€ å¹¶å‘æ•°: {max_workers} (CPUæ ¸å¿ƒæ•°: {cpu_count()})")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print(f"{'='*60}\n")

    # æ‰¹é‡å¤„ç† - å¹¶å‘æ‰§è¡Œ
    all_results = []
    start_time = datetime.now()

    # å‡†å¤‡å‚æ•°
    tasks = [(pdf_file, output_dir) for pdf_file in pdf_files]

    # ä½¿ç”¨è¿›ç¨‹æ± å¹¶å‘æ‰§è¡Œ
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {executor.submit(process_pdf_wrapper, task): task[0] for task in tasks}

        # æ”¶é›†ç»“æœï¼ˆæŒ‰å®Œæˆé¡ºåºï¼‰
        for future in as_completed(futures):
            result = future.result()
            all_results.append(result)

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print(f"\n{'='*60}")
    print("æµ‹è¯•æ±‡æ€»æŠ¥å‘Š")
    print(f"{'='*60}")

    success_count = sum(1 for r in all_results if r["status"] == "success")
    failed_count = len(all_results) - success_count

    total_tables = sum(r["stats"].get("è¡¨æ ¼æ•°", 0) for r in all_results if r["status"] == "success")
    total_pages = sum(r["stats"].get("æ€»é¡µæ•°", 0) for r in all_results if r["status"] == "success")

    # æŒ‰æ–‡ä»¶åæ’åºç»“æœ
    all_results.sort(key=lambda x: x["file_name"])

    summary = {
        "æµ‹è¯•æ—¶é—´": start_time.strftime("%Y-%m-%d %H:%M:%S"),
        "æ€»è€—æ—¶(ç§’)": round(duration, 2),
        "å¹¶å‘æ•°": max_workers,
        "å¤„ç†æ–‡ä»¶æ•°": len(all_results),
        "æˆåŠŸ": success_count,
        "å¤±è´¥": failed_count,
        "æˆåŠŸç‡": f"{success_count/len(all_results)*100:.1f}%" if all_results else "0%",
        "æ€»é¡µæ•°": total_pages,
        "æ€»è¡¨æ ¼æ•°": total_tables,
        "å¹³å‡æ¯ä»½è´¢æŠ¥è¡¨æ ¼æ•°": round(total_tables / success_count, 1) if success_count else 0,
        "å¹³å‡å¤„ç†é€Ÿåº¦(ç§’/ä»½)": round(duration / len(all_results), 2) if all_results else 0,
        "è¯¦ç»†ç»“æœ": all_results,
    }

    print(f"âœ… æˆåŠŸ: {success_count}/{len(all_results)} ({summary['æˆåŠŸç‡']})")
    print(f"âŒ å¤±è´¥: {failed_count}/{len(all_results)}")
    print(f"ğŸ“Š æ€»è¡¨æ ¼æ•°: {total_tables}")
    print(f"ğŸ“„ æ€»é¡µæ•°: {total_pages}")
    print(f"â±ï¸  æ€»è€—æ—¶: {duration:.1f}ç§’")
    print(f"âš¡ å¹³å‡é€Ÿåº¦: {summary['å¹³å‡å¤„ç†é€Ÿåº¦(ç§’/ä»½)']}ç§’/ä»½")

    # è¯¦ç»†ç»“æœè¡¨æ ¼
    print(f"\nè¯¦ç»†ç»“æœ:")
    print(f"{'æ–‡ä»¶å':<50} {'çŠ¶æ€':<10} {'é¡µæ•°':<8} {'è¡¨æ ¼æ•°':<8}")
    print("-" * 80)
    for r in all_results:
        status = "âœ… æˆåŠŸ" if r["status"] == "success" else "âŒ å¤±è´¥"
        pages = r["stats"].get("æ€»é¡µæ•°", 0)
        tables = r["stats"].get("è¡¨æ ¼æ•°", 0)
        file_name = r["file_name"][:47] + "..." if len(r["file_name"]) > 50 else r["file_name"]
        print(f"{file_name:<50} {status:<10} {pages:<8} {tables:<8}")

    # å¤±è´¥æ¡ˆä¾‹
    if failed_count > 0:
        print(f"\nå¤±è´¥çš„æ–‡ä»¶è¯¦æƒ…:")
        for r in all_results:
            if r["status"] == "failed":
                print(f"  âŒ {r['file_name']}")
                print(f"     é”™è¯¯: {r['error']}")

    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    report_file = output_dir / f"test_report_{start_time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")

    # æ‰“å°ä¸‹ä¸€æ­¥å»ºè®®
    print(f"\n{'='*60}")
    print("ä¸‹ä¸€æ­¥æ“ä½œå»ºè®®:")
    print(f"{'='*60}")
    if success_count > 0:
        print("1. æŸ¥çœ‹æå–ç»“æœ:")
        print(f"   cd {output_dir}")
        print(f"   cat <æ–‡ä»¶å>_extracted.json | head -100")
        print("2. åˆ†æè¡¨æ ¼è´¨é‡:")
        avg_tables = total_tables / success_count if success_count else 0
        if avg_tables < 5:
            print("   âš ï¸  å¹³å‡æ¯ä»½è´¢æŠ¥è¡¨æ ¼æ•°è¾ƒå°‘ï¼Œå»ºè®®:")
            print("   - æ£€æŸ¥PDFæ˜¯å¦ä¸ºæ‰«æä»¶")
            print("   - è€ƒè™‘å¢åŠ  Camelot æå–å™¨")
        elif avg_tables >= 5:
            print("   âœ… è¡¨æ ¼æå–æ•°é‡æ­£å¸¸")
        print("3. ä¸‹ä¸€æ­¥å¼€å‘:")
        print("   - ç´¢å¼•æ¨¡å—: å¯¹æå–çš„æ•°æ®å»ºç«‹å‘é‡ç´¢å¼•")
        print("   - æ£€ç´¢æ¨¡å—: å®ç°è¯­ä¹‰æ£€ç´¢")
    else:
        print("âš ï¸  æ‰€æœ‰æ–‡ä»¶æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
        print("1. PDFæ–‡ä»¶æ˜¯å¦æŸåæˆ–åŠ å¯†")
        print("2. ä¾èµ–æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("3. æŸ¥çœ‹é”™è¯¯æ—¥å¿—")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
